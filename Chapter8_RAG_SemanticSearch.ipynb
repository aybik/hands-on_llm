{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iuzcr8pEslXa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.2.5 faiss-cpu==1.8.0 cohere==5.5.8 langchain-community==0.2.5 rank_bm25==0.2.2 sentence-transformers==3.0.1\n",
        "!pip install llama-cpp-python==0.2.78  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CRPknqe6UA"
      },
      "source": [
        "There‚Äôs a lot of research on how to best use language models for search. Three\n",
        "broad categories of these models are dense retrieval, reranking, and RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf4j4OrIuOqp"
      },
      "source": [
        "# Dense Retrieval Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1YT-JnufAbR"
      },
      "source": [
        "Dense retrieval systems rely on the concept of embeddings, the same concept\n",
        "we‚Äôve encountered in the previous chapters, and turn the search problem into\n",
        "retrieving the nearest neighbors of the search query (after both the query and\n",
        "the documents are converted into embeddings). Figure 8-1 shows how dense\n",
        "retrieval takes a search query, consults its archive of texts, and outputs a set of\n",
        "relevant results.\n",
        "\n",
        "Let‚Äôs take a look at a dense retrieval example by using Cohere to search the Wikipedia\n",
        "page for the film Interstellar. In this example, we will do the following:\n",
        "1. Get the text we want to make searchable and apply some light processing to\n",
        "chunk it into sentences.\n",
        "2. Embed the sentences.\n",
        "3. Build the search index.\n",
        "4. Search and see the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc4GPi92uRJW"
      },
      "source": [
        "## 1. Getting the text archive and chunking it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wDrS_FkTtc4r"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cohere\n",
        "\n",
        "# Paste your API key here. Remember to not share publicly\n",
        "api_key = ''\n",
        "\n",
        "# Create and retrieve a Cohere API key from os.cohere.ai\n",
        "co = cohere.Client(api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4zCoYLMTu7XL"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
        "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
        "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
        "\n",
        "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
        "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
        "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
        "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
        "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
        "\n",
        "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
        "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
        "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
        "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
        "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
        "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
        "\n",
        "# Split into a list of sentences\n",
        "texts = text.split('.')\n",
        "\n",
        "# Clean up to remove empty spaces and new lines\n",
        "texts = [t.strip(' \\n') for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan',\n",
              " 'It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine',\n",
              " 'Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind',\n",
              " 'Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007',\n",
              " 'Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar',\n",
              " 'Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm',\n",
              " 'Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles',\n",
              " 'Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects',\n",
              " 'Interstellar premiered on October 26, 2014, in Los Angeles',\n",
              " 'In the United States, it was first released on film stock, expanding to venues using digital projectors',\n",
              " 'The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014',\n",
              " 'It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight',\n",
              " 'It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics',\n",
              " 'Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time',\n",
              " 'Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Embedding the Text Chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let‚Äôs now embed the texts. We‚Äôll send them to the Cohere\n",
        "API, and get back a vector for each text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the embeddings\n",
        "response = co.embed(\n",
        "  texts=texts,\n",
        "  input_type=\"search_document\",\n",
        ").embeddings\n",
        "\n",
        "embeds = np.array(response)\n",
        "print(embeds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This outputs (15, 4096), which indicates that we have 15 vectors, each one of size\n",
        "4,096."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building The Search Index\n",
        "Before we can search, we need to build a search index.\n",
        "An index stores the embeddings and is optimized to quickly retrieve the nearest\n",
        "neighbors even if we have a very large number of points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "dim = embeds.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(np.float32(embeds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Search the index\n",
        "We can now search the dataset using any query we want. We simply\n",
        "embed the query and present its embedding to the index, which will retrieve the most\n",
        "similar sentence from the Wikipedia article.\n",
        "Let‚Äôs define our search function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def search(query, number_of_results=3):\n",
        "\n",
        "  # 1. Get the query's embedding\n",
        "  query_embed = co.embed(texts=[query],\n",
        "                input_type=\"search_query\",).embeddings[0]\n",
        "\n",
        "  # 2. Retrieve the nearest neighbors\n",
        "  distances , similar_item_ids = index.search(np.float32([query_embed]), number_of_results)\n",
        "\n",
        "  # 3. Format the results\n",
        "  texts_np = np.array(texts) # Convert texts list to numpy for easier indexing\n",
        "  results = pd.DataFrame(data={'texts': texts_np[similar_item_ids[0]],\n",
        "                              'distance': distances[0]})\n",
        "\n",
        "  # 4. Print and return the results\n",
        "  print(f\"Query:'{query}'\\nNearest neighbors:\")\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"how precise was the science\"\n",
        "results = search(query)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| texts | distance |\n",
        "| --- | --- |\n",
        "| It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics | 10757.379883 |\n",
        "| Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar | 11566.131836 |\n",
        "| Interstellar uses extensive practical and mini... | 11922.833008 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first result has the least distance, and so is the most similar to the query. Looking\n",
        "at it, it answers the question perfectly. Notice that this wouldn‚Äôt have been possible if\n",
        "we were only doing keyword search because the top result did not include the same\n",
        "keywords in the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Disadvantages of Dense Retrieval\n",
        "\n",
        "#### 1. Irrelevant Results When the Answer Is Missing\n",
        "- If the retrieved texts do not contain the answer, dense retrieval still returns results based on similarity scores.\n",
        "- **Example**: A query like *\"What is the mass of the moon?\"* might return unrelated results about movies or cinematography.\n",
        "- This happens because dense retrieval is based on embeddings and similarity, not direct keyword matching.\n",
        "\n",
        "#### 2. Handling Long Texts (Chunking Challenges)\n",
        "- **Transformer models have a limited context size**, restricting the number of tokens they can process at once.\n",
        "- Long documents need to be split into smaller chunks, but how this is done affects retrieval quality.\n",
        "\n",
        "##### Chunking Strategies:\n",
        "- **One Vector per Document:**\n",
        "  - Embedding only a representative part (e.g., title or introduction) leaves out a lot of information.\n",
        "  - Averaging embeddings from multiple chunks compresses information, reducing accuracy.\n",
        "- **Multiple Vectors per Document:**\n",
        "  - Documents are split into smaller chunks, and each chunk is embedded separately.\n",
        "  - This improves retrieval accuracy but increases storage and computational requirements.\n",
        "\n",
        "##### Best Practices for Chunking:\n",
        "- **Sentence-level chunks**: Too granular, losing context.\n",
        "- **Paragraph-level chunks**: Works well if paragraphs are concise.\n",
        "- **Overlapping chunks**: Improves context retention by including nearby text.\n",
        "- **Adding titles or surrounding text**: Helps provide better contextual understanding.\n",
        "\n",
        "As the field advances, more dynamic and LLM-based chunking methods are expected to emerge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A reranker takes in the search query and a number of search results, and returns\n",
        "the optimal ordering of these documents so the most relevant ones to the query are\n",
        "higher in ranking. Cohere‚Äôs Rerank endpoint is a simple way to start using a first\n",
        "reranker. We simply pass it the query and texts and get the results back. We don‚Äôt\n",
        "need to train or tune it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"how precise was the science\"\n",
        "results = co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
        "results.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, result in enumerate(results.results):\n",
        "    print(idx, result.relevance_score , result.document.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0 0.1698185 It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics\n",
        "\n",
        "1 0.07004896 The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014\n",
        "\n",
        "2 0.0043994132 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Difference Between Dense Retrieval and Reranking\n",
        "\n",
        "| Feature         | **Dense Retrieval** | **Reranking** |\n",
        "|---------------|-----------------|----------------|\n",
        "| **Purpose** | Retrieves the most relevant documents from a large corpus. | Reorders the retrieved documents to improve ranking accuracy. |\n",
        "| **Process** | Uses vector embeddings and similarity search to retrieve top-k candidates. | Takes the top-k retrieved documents and refines their ranking based on a more sophisticated scoring model. |\n",
        "| **Speed** | Fast, optimized for large-scale retrieval. | Slower, as it applies a second-stage ranking process. |\n",
        "| **Computational Cost** | Lower, as it relies on approximate nearest neighbor (ANN) search. | Higher, as it often uses transformer models or deep learning for precise ranking. |\n",
        "| **Model Type** | Typically based on **bi-encoder** architectures (e.g., SBERT, DPR). | Often uses **cross-encoder** architectures (e.g., BERT-based rerankers). |\n",
        "| **Strengths** | Efficient for large document collections; good for first-stage retrieval. | Improves precision by considering fine-grained contextual relationships between query and documents. |\n",
        "| **Weaknesses** | May retrieve irrelevant results if the embedding similarity is misleading. | Computationally expensive and requires processing each query-document pair separately. |\n",
        "| **Example Use Case** | Finding a set of potentially relevant articles for a query. | Refining the ranking of retrieved articles to show the most relevant ones at the top. |\n",
        "\n",
        "#### How They Work Together\n",
        "1. **Dense Retrieval (First Stage)**: Quickly retrieves the top-k most relevant documents using embeddings.\n",
        "2. **Reranking (Second Stage)**: Uses a more detailed model to reorder these documents for better relevance.\n",
        "\n",
        "This two-step approach balances **efficiency** (dense retrieval) and **accuracy** (reranking), making it a common pipeline in information retrieval systems. üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A basic RAG pipeline is made up of a search step followed by a grounded\n",
        "generation step where the LLM is prompted with the question and the information\n",
        "retrieved from the search step.\n",
        "\n",
        "RAG systems incorporate search capabilities in addition to generation capabilities.\n",
        "They can be seen as an improvement to generation systems because they reduce\n",
        "their hallucinations and improve their factuality. They also enable use cases of ‚Äúchat\n",
        "with my data‚Äù that consumers and companies can use to ground an LLM on internal\n",
        "company data, or a specific data source of interest (e.g., chatting with a book).\n",
        "This also extends to search systems. More search engines are incorporating an LLM\n",
        "to summarize results or answer questions submitted to the search engine. Examples\n",
        "include Perplexity, Microsoft Bing AI, and Google Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hands-on_llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
